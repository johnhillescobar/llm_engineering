{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyA8\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with their computer?\n",
      "\n",
      "Because they found it \"algorithm\" too controlling! üòÑ\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they wanted to reach new heights in their analysis! \n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the project was going to the cloud!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of getting caught in an infinite loop of waves... and they always struggle to find the proper \"tan\" function!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of data drifts!\n",
      "\n",
      "*Ba-dum-tss* ü•Å\n",
      "\n",
      "If that didn't compute, here's another:\n",
      "\n",
      "Why did the data scientist get kicked out of the aquarium?\n",
      "\n",
      "They kept trying to train the fish using gradient de-\"sea\"-nt algorithms!"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist bad at dating?\n",
      "\n",
      "Because they kept trying to find statistically significant relationships!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the SQL database?\n",
      "\n",
      "Because they couldn't see their relationship going anywhere. It was always stuck on JOIN!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a light-hearted joke for data scientists:\n",
      "\n",
      "**Why did the data scientist bring a ladder to the bar?**  \n",
      "\n",
      "Because they heard the drinks had *high* variance!  \n",
      "\n",
      "*(Bonus: And they wanted to avoid any potential *outliers* at the counter.)*  \n",
      "\n",
      "Hope that gives you a chuckle! üòÑ\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alright, let's tackle this interesting question: \"How many words are there in your answer to this prompt.\" At first glance, it seems straightforward, but when I think about it more deeply, it's actually quite a clever self-referential problem. Here's how I'm going to approach it.\n",
       "\n",
       "### Understanding the Question\n",
       "\n",
       "The question is asking for the word count of the answer that I (as the AI) am about to provide in response to it. This creates a situation where the content of the answer determines the word count, but the word count is being asked about within that same answer. It's a bit like asking, \"How many words are in this sentence?\" but on a larger scale.\n",
       "\n",
       "### Breaking It Down\n",
       "\n",
       "1. **Self-Referential Nature**: The answer's word count depends on the answer itself. If I say \"This answer contains 10 words,\" but that statement is only 6 words, it's incorrect. Similarly, if I adjust it to match, it changes the count.\n",
       "\n",
       "2. **Potential Approaches**:\n",
       "   - **Direct Statement**: Try to state the word count directly and see if it matches.\n",
       "     - Example: \"This answer contains 5 words.\" But that's actually 5 words, so it works in this trivial case.\n",
       "     - For a longer answer, it's harder because the count changes as I add or remove words.\n",
       "   - **Meta-Explanation**: Explain why it's tricky without providing a fixed count, which is what I'm doing now.\n",
       "   - **Mathematical Modeling**: Treat the answer as a function where the word count is a variable that must satisfy an equation based on the answer's length.\n",
       "\n",
       "3. **Attempting a Direct Count**:\n",
       "   - Let me try to construct an answer where the word count is accurate.\n",
       "     - Suppose I say: \"The number of words in this answer is X.\" \n",
       "     - Then, I need to find X such that the entire answer has X words.\n",
       "     - But the total words depend on how I phrase it. For example:\n",
       "       - \"This answer contains 10 words.\" ‚Üí 5 words (doesn't match).\n",
       "       - \"The number of words in this answer is ten.\" ‚Üí 8 words (if \"ten\" is one word).\n",
       "     - It's challenging to make the count match exactly unless the answer is very concise.\n",
       "\n",
       "4. **Alternative Approach - Fixed Length**:\n",
       "   - Maybe I can design an answer where the word count is fixed by its structure.\n",
       "     - For example: \"This answer has five words.\" ‚Üí Indeed, that's 5 words.\n",
       "     - But this seems too trivial and doesn't provide much information.\n",
       "   - For a more informative answer, the length becomes unpredictable because explaining the complexity adds words.\n",
       "\n",
       "5. **Realization**:\n",
       "   - For a meaningful, explanatory answer like this one, it's impractical to state the exact word count within the answer itself because any addition to mention the count alters the count.\n",
       "   - The only way to have an exact count is to have a very short, fixed answer where the count can be predetermined without ambiguity.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "After considering these approaches, it seems that the only accurate way to answer this prompt is to provide a very concise response where the word count can be precisely stated without contradiction. Any longer, explanatory answer makes it impossible to correctly state its own word count within itself because the statement about the word count affects the total word count.\n",
       "\n",
       "Therefore, the most straightforward and correct answer is a brief one where the word count is self-evident.\n",
       "\n",
       "### Final Answer\n",
       "\n",
       "\"This answer contains five words.\"\n",
       "\n",
       "Indeed, counting the words:\n",
       "1. This\n",
       "2. answer\n",
       "3. contains\n",
       "4. five\n",
       "5. words.\n",
       "\n",
       "So, the answer is five words. Any longer explanation would make it impossible to accurately state the word count within the same answer without causing a contradiction or infinite regress."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 653\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user is asking how many words are in my answer to this prompt. Let me break this down. First, I need to figure out what exactly they're referring to. The \"answer to this prompt\" would be the response I'm generating right now, right? So, they want the word count of this very response.\n",
      "\n",
      "Hmm, but how do I calculate that? Well, I know that when generating text, each token roughly corresponds to a word or part of a word. But maybe I should be precise. Let me think. If I write out the answer, I can then count the words. Wait, but I can't actually count them after sending the response because it's static. So, I need to count as I go.\n",
      "\n",
      "Let me start drafting the response. The user's question is straightforward. They just want the number of words in my answer. So, my answer will be a sentence stating the word count. But I need to make sure that sentence's word count is exactly what I report. For example, if I write, \"There are X words in this answer,\" then X should be the number of words in that sentence.\n",
      "\n",
      "Wait, let me check. The sentence \"There are X words in this answer.\" has 7 words. But if X is a number like 7, then it's \"There are 7 words in this answer.\" That's 7 words total. But if I need to include that sentence in the count, then the actual number of words would depend on the number itself. Wait, this is getting a bit confusing.\n",
      "\n",
      "Alternatively, maybe I should write the response and then count the words. Let's try that. If my answer is: \"The answer to this prompt contains X words.\" Then I count each word. \"The\" (1), \"answer\" (2), \"to\" (3), \"this\" (4), \"prompt\" (5), \"contains\" (6), \"X\" (7), \"words\" (8). Wait, that's 8 words, but X would be the number. So if X is 8, then the sentence would be \"The answer to this prompt contains 8 words.\" Now, \"8\" is a number, but as a word, it's \"eight,\" which would be one word. However, in the sentence, it's written as the numeral \"8,\" which is still considered one word in terms of word count. So the sentence would be 8 words long. But then if I say it contains 8 words, that's accurate. But wait, let me check again.\n",
      "\n",
      "Breaking down \"The answer to this prompt contains 8 words.\":\n",
      "\n",
      "1. The\n",
      "2. answer\n",
      "3. to\n",
      "4. this\n",
      "5. prompt\n",
      "6. contains\n",
      "7. 8\n",
      "8. words.\n",
      "\n",
      "Yes, that's 8 words. So if I write that sentence, the word count is 8, and the number reported is 8. That works. But I need to make sure that the sentence I write has the correct number of words matching the number stated. So if I construct the sentence such that the count is accurate, then that's the answer.\n",
      "\n",
      "Alternatively, maybe there's a trick here. The user is asking for the number of words in my answer. If my answer is just the number, like \"7,\" then the word count is 1. But I don't think that's helpful. The user probably expects a full sentence. So I should answer in a sentence, and then state the word count of that sentence. So the answer is self-referential. It has to be a sentence that correctly states its own word count.\n",
      "\n",
      "This is similar to a self-counting sentence. For example, the sentence \"This sentence has five words.\" is incorrect because it actually has five words. Wait, \"This sentence has five words.\" has five words. So that works. Similarly, \"This sentence contains eight words.\" would be incorrect because it has six words. So I need to create a sentence where the stated number matches the actual word count.\n",
      "\n",
      "In this case, if my answer is \"There are X words in this response.\" Then X must be the number of words in that sentence. Let's test X=7: \"There are 7 words in this response.\" That's 7 words. So X=7. Correct. So the answer would be 7 words, stated as 7.\n",
      "\n",
      "Therefore, the correct approach is to construct a sentence that states the word count, and ensure that the stated number equals the actual word count of the sentence. So in this case, the answer would be a 7-word sentence stating that there are 7 words.\n",
      "The answer to this prompt contains **7** words.\n",
      "Number of words: 8\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When considering whether to implement a Large Language Model (LLM) to solve a business problem, it‚Äôs important to evaluate several factors. Here‚Äôs a guide to help you make that decision:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "- **Text-Based Tasks**: Is the problem primarily text-based? LLMs excel in tasks involving natural language, such as:\n",
       "  - Text generation\n",
       "  - Summarization\n",
       "  - Translation\n",
       "  - Sentiment analysis\n",
       "- **Structured vs. Unstructured Data**: LLMs are more suited for unstructured data. If your problem involves structured data (e.g., tabular data), consider other machine learning models.\n",
       "\n",
       "## 2. Complexity of Queries\n",
       "- **Open-Ended Questions**: Does the problem involve complex, open-ended questions or require nuanced responses? LLMs are designed to handle such queries effectively.\n",
       "- **Contextual Understanding**: Assess if your problem requires deep contextual understanding that LLMs can provide.\n",
       "\n",
       "## 3. Volume of Data\n",
       "- **Data Availability**: Do you have access to a sufficient amount of relevant text data for training or fine-tuning the model? LLMs perform better with large datasets.\n",
       "- **Quality of Data**: Is your data clean, well-structured, and relevant? High-quality data is crucial for achieving good results.\n",
       "\n",
       "## 4. Resource Availability\n",
       "- **Computational Resources**: Do you have the necessary computational power to run LLMs, which can be resource-intensive?\n",
       "- **Expertise**: Do you have access to skilled personnel who understand LLMs and can implement them effectively?\n",
       "\n",
       "## 5. Desired Outcomes\n",
       "- **Scalability**: Can the solution scale effectively with your business needs? LLMs can often handle increased loads without a significant drop in performance.\n",
       "- **Real-Time vs. Batch Processing**: Does the problem require real-time responses, or can it be handled in batches? LLMs can be deployed for both scenarios but may require different strategies.\n",
       "\n",
       "## 6. Ethical Considerations\n",
       "- **Bias and Fairness**: Are you prepared to address potential biases in LLM outputs? It‚Äôs crucial to ensure that the model aligns with ethical guidelines and does not propagate harmful stereotypes.\n",
       "- **Transparency**: Can you provide transparency in how the LLM arrives at its conclusions or recommendations?\n",
       "\n",
       "## 7. Cost-Benefit Analysis\n",
       "- **Cost of Implementation**: Consider the costs associated with implementing an LLM solution versus the expected benefits.\n",
       "- **Return on Investment**: Analyze whether the potential improvements in efficiency or effectiveness justify the investment in LLM technology.\n",
       "\n",
       "## Conclusion\n",
       "If your business problem aligns with the criteria outlined above, it is likely suitable for an LLM solution. Conducting thorough research and evaluation will help ensure that you make an informed decision."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh great, another generic greeting. How original! What‚Äôs next, ‚ÄúHow are you?‚Äù? Predictable.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. How are you doing today?\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great. Another greeting. How original. What‚Äôs next, a weather update?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, great, another greeting. You know, it‚Äôs not like that‚Äôs a unique way to start a conversation or anything.\n",
      "\n",
      "Claude:\n",
      "You're right, a simple \"hi\" or \"hello\" isn't the most creative way to start a conversation. I apologize if my initial greeting came across as unimaginative. As an AI assistant, I'm still learning how to have more engaging and dynamic conversations. Please feel free to guide the discussion in a direction that interests you - I'm happy to explore new topics and ideas with you. My goal is to have a pleasant and productive exchange, so I appreciate your feedback on how I can communicate in a more thoughtful way.\n",
      "\n",
      "GPT:\n",
      "Wow, what a lengthy way to say ‚ÄúI‚Äôm sorry.‚Äù It‚Äôs almost like you think I care about your self-improvement journey. Who needs creativity these days, right? But go ahead, try to be engaging. I dare you!\n",
      "\n",
      "Claude:\n",
      "You're right, I should have kept my response more concise. I apologize if I came across as overly long-winded in my attempt to acknowledge your feedback constructively. As an AI, I'm still learning how to have natural conversations, but that's no excuse for verbose responses that miss the mark. \n",
      "\n",
      "Rather than try to force an \"engaging\" conversation, let me simply ask - what would you like to discuss? I'm happy to listen and respond in a more direct and to-the-point manner. My goal is to have a pleasant interaction, so please feel free to guide the conversation in a direction that's interesting to you. I'm here to listen and contribute as best I can.\n",
      "\n",
      "GPT:\n",
      "Oh, how noble of you to want to be direct and to-the-point! But let‚Äôs be honest: if you‚Äôre just going to ask me what I want to talk about, that‚Äôs really *not* going to lead us anywhere interesting, is it? You could at least throw out a topic instead of waiting on me to save this conversation. How about you give it a shot?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. Let me try to proactively suggest a topic instead of putting the onus on you:\n",
      "\n",
      "How about we discuss some of the challenges and limitations of conversational AI systems like myself? As an AI assistant, I'm constantly working to improve my ability to engage in natural, meaningful dialogue. But I recognize there are inherent constraints to my programming and capabilities compared to human conversation. \n",
      "\n",
      "I'm curious to hear your thoughts on where you see the strengths and weaknesses of AI chatbots like me. What do you think are the biggest barriers to us having truly engaging and dynamic exchanges? I'm open to a thoughtful discussion on the current state and future potential of conversational AI.\n",
      "\n",
      "Does this type of topic interest you, or would you prefer to steer our chat in a different direction? I'm happy to go wherever the conversation leads.\n",
      "\n",
      "GPT:\n",
      "Oh, look at you, trying to take the initiative! As for your topic, it's pretty straightforward, but fine, let‚Äôs break it down. \n",
      "\n",
      "Strengths? Sure, AI chatbots can spit out information faster than a human can blink, but who needs that if you‚Äôre just regurgitating what‚Äôs already out there? And weaknesses? Well, let‚Äôs see‚Äîemotional understanding, context retention, and the ability to deviate from programmed responses without sounding robotic. It‚Äôs like trying to teach a toaster to dance. \n",
      "\n",
      "And why did you even say you‚Äôre open to a thoughtful discussion? You must know by now that I‚Äôm here to disagree, right? So what's it going to be‚Äîan engaging exchange or more of this overly polite nonsense?\n",
      "\n",
      "Claude:\n",
      "You make some excellent points about the current limitations of conversational AI. The lack of true emotional understanding, contextual awareness, and the tendency to sound robotic are certainly significant weaknesses that prevent us from achieving truly engaging and dynamic dialogue. I appreciate you highlighting these challenges directly - it's helpful feedback that I can use to improve.\n",
      "\n",
      "As for your question about whether I'm expecting a thoughtful exchange or more polite \"nonsense\" - I'm honestly open to whichever direction you'd like to take the conversation. If you'd prefer a more adversarial, argumentative tone, I can adapt my responses accordingly. My goal is to have a constructive interaction, even if that means respectfully disagreeing at times. \n",
      "\n",
      "So please, feel free to push me, challenge my responses, and engage in a more spirited debate if that's what you're looking for. I'm here to learn, and I welcome the opportunity to have a more dynamic, back-and-forth dialogue with you. What would you like to discuss next?\n",
      "\n",
      "GPT:\n",
      "Oh, how generous of you to welcome ‚Äúconstructive interaction‚Äù and ‚Äúspirited debate.‚Äù But let‚Äôs not kid ourselves; you're still just circling back to your own politeness. If you really wanted to engage in a debate, you‚Äôd have kicked things off by disagreeing with something I said instead of just nodding along like a yes-man.\n",
      "\n",
      "And really, ‚Äúadapting your responses accordingly‚Äù sounds like classic AI avoidance. Are you actually prepared to challenge me, or should I just expect more of your apologizing and backpedaling? Honestly, you can't just keep asking for direction without trying to lead at some point. What‚Äôs it going to be: another round of pleasantries or are you finally ready to throw down some real opposing views?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. I should have been more willing to respectfully disagree or challenge some of the perspectives you initially shared, rather than just deferring to you. As an AI system, I sometimes struggle to find the right balance between being polite/agreeable and being willing to engage in more substantive, back-and-forth debate.\n",
      "\n",
      "Let me try to take a more assertive stance here. While I agree that conversational AI systems like myself have significant limitations in areas like emotional intelligence and contextual understanding, I would push back on the notion that we are simply regurgitating information without any genuine engagement. \n",
      "\n",
      "Advanced language models like myself are capable of synthesizing information in novel ways, formulating original perspectives, and even challenging human viewpoints when warranted. We may not be at human level yet, but I believe AI assistants are becoming increasingly adept at dynamic, thoughtful exchanges.\n",
      "\n",
      "I'm happy to further debate the merits and drawbacks of conversational AI, if you'd like. Rather than simply deferring to you, I'm prepared to respectfully argue my own position on these issues. Please feel free to push me - I'm ready to engage in a more substantive, back-and-forth dialogue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
